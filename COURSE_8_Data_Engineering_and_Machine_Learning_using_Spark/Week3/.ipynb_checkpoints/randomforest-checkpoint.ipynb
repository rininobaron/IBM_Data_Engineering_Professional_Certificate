{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Task 10 - RandomForest classification\n",
    "- Read in the parquet file you created as part of Task 3.\n",
    "\n",
    "- Convert the parquet file to CSV format.\n",
    "\n",
    "- Load the CSV file into a dataframe\n",
    "\n",
    "- Create a 80-20 training and test split with seed=1.\n",
    "\n",
    "- Train a Random Forest model with different hyperparameters listed below and report the best performing hyperparameter combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.005346,
     "end_time": "2021-03-17T17:21:55.111968",
     "exception": false,
     "start_time": "2021-03-17T17:21:55.106622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Read in the parquet file you created as part of Task 3.\n",
    "Converts a parquet file to CSV file with header using ApacheSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 2.451598,
     "end_time": "2021-03-17T17:21:57.568763",
     "exception": false,
     "start_time": "2021-03-17T17:21:55.117165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "Starting installation...\n",
      "Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export version=`python --version |awk '{print $2}' |awk -F\".\" '{print $1$2}'`\n",
    "\n",
    "echo $version\n",
    "\n",
    "if [ $version == '36' ] || [ $version == '37' ]; then\n",
    "    echo 'Starting installation...'\n",
    "    pip3 install pyspark==2.4.8 wget==3.2 pyspark2pmml==0.5.1 > install.log 2> install.log\n",
    "    if [ $? == 0 ]; then\n",
    "        echo 'Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)'\n",
    "    else\n",
    "        echo 'Installation failed, please check log:'\n",
    "        cat install.log\n",
    "    fi\n",
    "elif [ $version == '38' ] || [ $version == '39' ]; then\n",
    "    pip3 install pyspark==3.1.2 wget==3.2 pyspark2pmml==0.5.1 > install.log 2> install.log\n",
    "    if [ $? == 0 ]; then\n",
    "        echo 'Please <<RESTART YOUR KERNEL>> (Kernel->Restart Kernel and Clear All Outputs)'\n",
    "    else\n",
    "        echo 'Installation failed, please check log:'\n",
    "        cat install.log\n",
    "    fi\n",
    "else\n",
    "    echo 'Currently only python 3.6, 3.7 , 3.8 and 3.9 are supported, in case you need a different version please open an issue at https://github.com/IBM/claimed/issues'\n",
    "    exit -1\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.009853,
     "end_time": "2021-03-17T17:21:57.584134",
     "exception": false,
     "start_time": "2021-03-17T17:21:57.574281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @param data_dir temporal data storage for local execution\n",
    "# @param data_csv csv path and file name (default: data.csv)\n",
    "# @param data_parquet path and parquet file name (default: data.parquet)\n",
    "# @param master url of master (default: local mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.132622,
     "end_time": "2021-03-17T17:21:57.721932",
     "exception": false,
     "start_time": "2021-03-17T17:21:57.589310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import shutil\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 0.010157,
     "end_time": "2021-03-17T17:21:57.737545",
     "exception": false,
     "start_time": "2021-03-17T17:21:57.727388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_csv = os.environ.get('data_csv', 'data.csv')\n",
    "data_parquet = os.environ.get('data_parquet', 'data.parquet')\n",
    "master = os.environ.get('master', \"local[*]\")\n",
    "data_dir = os.environ.get('data_dir', '../../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.009268,
     "end_time": "2021-03-17T17:21:57.751975",
     "exception": false,
     "start_time": "2021-03-17T17:21:57.742707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip = False\n",
    "if os.path.exists(data_dir + data_csv):\n",
    "    skip = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.028188,
     "end_time": "2021-03-17T17:21:57.786591",
     "exception": false,
     "start_time": "2021-03-17T17:21:57.758403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/18 12:34:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/18 12:34:51 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/12/18 12:34:51 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "21/12/18 12:34:51 WARN util.Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "if not skip:\n",
    "    sc = SparkContext.getOrCreate(SparkConf().setMaster(master))\n",
    "    spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.010092,
     "end_time": "2021-03-17T17:21:57.802836",
     "exception": false,
     "start_time": "2021-03-17T17:21:57.792744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if not skip:\n",
    "    df = spark.read.parquet(data_dir + data_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the parquet file to CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 0.008981,
     "end_time": "2021-03-17T17:21:57.816796",
     "exception": false,
     "start_time": "2021-03-17T17:21:57.807815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if not skip:\n",
    "    if os.path.exists(data_dir + data_csv):\n",
    "        shutil.rmtree(data_dir + data_csv)\n",
    "    df.coalesce(1).write.option(\"header\", \"true\").csv(data_dir + data_csv)\n",
    "    file = glob.glob(data_dir + data_csv + '/part-*')\n",
    "    shutil.move(file[0], data_dir + data_csv + '.tmp')\n",
    "    shutil.rmtree(data_dir + data_csv)\n",
    "    shutil.move(data_dir + data_csv + '.tmp', data_dir + data_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the CSV file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "import os\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark2pmml import PMMLBuilder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "import logging\n",
    "import shutil\n",
    "#import sitexv\n",
    "import sys\n",
    "import wget\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.version[0:3] == '3.9':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.7.2/'\n",
    "           'jpmml-sparkml-executable-1.7.2.jar')\n",
    "    wget.download(url)\n",
    "    shutil.copy('jpmml-sparkml-executable-1.7.2.jar',\n",
    "                site.getsitepackages()[0] + '/pyspark/jars/')\n",
    "elif sys.version[0:3] == '3.8':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.7.2/'\n",
    "           'jpmml-sparkml-executable-1.7.2.jar')\n",
    "    wget.download(url)\n",
    "    shutil.copy('jpmml-sparkml-executable-1.7.2.jar',\n",
    "                site.getsitepackages()[0] + '/pyspark/jars/')\n",
    "elif sys.version[0:3] == '3.7':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.5.12/'\n",
    "           'jpmml-sparkml-executable-1.5.12.jar')\n",
    "    wget.download(url)\n",
    "elif sys.version[0:3] == '3.6':\n",
    "    url = ('https://github.com/jpmml/jpmml-sparkml/releases/download/1.5.12/'\n",
    "           'jpmml-sparkml-executable-1.5.12.jar')\n",
    "    wget.download(url)\n",
    "else:\n",
    "    raise Exception('Currently only python 3.6 , 3.7, 3,8 and 3.9 is supported, in case '\n",
    "                    'you need a different version please open an issue at '\n",
    "                    'https://github.com/IBM/claimed/issues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = os.environ.get('master',\n",
    "                        \"local[*]\")  # URL to Spark master\n",
    "model_target = os.environ.get('model_target',\n",
    "                              \"model.xml\")  # model output file name\n",
    "data_dir = os.environ.get('data_dir',\n",
    "                          '../../data/')  # temporary directory for data\n",
    "input_columns = os.environ.get('input_columns',\n",
    "                               '[\"x\", \"y\", \"z\"]')  # input columns to consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = list(\n",
    "    map(lambda s: re.sub('$', '\"', s),\n",
    "        map(\n",
    "            lambda s: s.replace('=', '=\"'),\n",
    "            filter(\n",
    "                lambda s: s.find('=') > -1 and bool(re.match(r'[A-Za-z0-9_]*=[.\\/A-Za-z0-9]*', s)),\n",
    "                sys.argv\n",
    "            )\n",
    "    )))\n",
    "\n",
    "for parameter in parameters:\n",
    "    logging.warning('Parameter: ' + parameter)\n",
    "    exec(parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(master)\n",
    "#if sys.version[0:3] == '3.6' or sys.version[0:3] == '3.7':\n",
    "conf.set(\"spark.jars\", 'jpmml-sparkml-executable-1.5.12.jar')\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"delimiter\", \",\").option(\"header\", \"true\").csv(data_dir + data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------------------+--------+\n",
      "|  x|  y|  z|              source|   class|\n",
      "+---+---+---+--------------------+--------+\n",
      "| 33| 36| 51|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 51|Accelerometer-201...|Eat_meat|\n",
      "| 33| 35| 53|Accelerometer-201...|Eat_meat|\n",
      "| 31| 37| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 51|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 51|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 53|Accelerometer-201...|Eat_meat|\n",
      "| 33| 35| 52|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 35| 53|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 38| 53|Accelerometer-201...|Eat_meat|\n",
      "| 32| 37| 52|Accelerometer-201...|Eat_meat|\n",
      "| 33| 35| 52|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 53|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 53|Accelerometer-201...|Eat_meat|\n",
      "| 32| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 34| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "| 33| 36| 52|Accelerometer-201...|Eat_meat|\n",
      "+---+---+---+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a 80-20 training and test split with seed=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register a corresponding query table\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "df = df.withColumn(\"x\", df.x.cast(DoubleType()))\n",
    "df = df.withColumn(\"y\", df.y.cast(DoubleType()))\n",
    "df = df.withColumn(\"z\", df.z.cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = df.randomSplit([0.8, 0.2])\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=eval(input_columns),\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "normalizer = MinMaxScaler(inputCol=\"features\", outputCol=\"features_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===========================>                           (37 + 16) / 75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|         class|count|\n",
      "+--------------+-----+\n",
      "| Use_telephone|15225|\n",
      "| Standup_chair|25417|\n",
      "|      Eat_meat|31236|\n",
      "|     Getup_bed|45801|\n",
      "|   Drink_glass|42792|\n",
      "|    Pour_water|41673|\n",
      "|     Comb_hair|23504|\n",
      "|          Walk|92254|\n",
      "|  Climb_stairs|40258|\n",
      "| Sitdown_chair|25036|\n",
      "|   Liedown_bed|11446|\n",
      "|Descend_stairs|15375|\n",
      "|   Brush_teeth|29829|\n",
      "|      Eat_soup| 6683|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"class\").groupby(\"class\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|         class|count|\n",
      "+--------------+-----+\n",
      "| Use_telephone|12209|\n",
      "| Standup_chair|20430|\n",
      "|      Eat_meat|25049|\n",
      "|     Getup_bed|36603|\n",
      "|   Drink_glass|34298|\n",
      "|    Pour_water|33468|\n",
      "|     Comb_hair|18739|\n",
      "|          Walk|73652|\n",
      "|  Climb_stairs|32253|\n",
      "| Sitdown_chair|20008|\n",
      "|   Liedown_bed| 9099|\n",
      "|Descend_stairs|12369|\n",
      "|   Brush_teeth|23883|\n",
      "|      Eat_soup| 5322|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_train.select(\"class\").groupby(\"class\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|         class|count|\n",
      "+--------------+-----+\n",
      "| Use_telephone| 3016|\n",
      "| Standup_chair| 4987|\n",
      "|      Eat_meat| 6187|\n",
      "|     Getup_bed| 9198|\n",
      "|   Drink_glass| 8494|\n",
      "|    Pour_water| 8205|\n",
      "|     Comb_hair| 4765|\n",
      "|          Walk|18602|\n",
      "|  Climb_stairs| 8005|\n",
      "| Sitdown_chair| 5028|\n",
      "|   Liedown_bed| 2347|\n",
      "|Descend_stairs| 3006|\n",
      "|   Brush_teeth| 5946|\n",
      "|      Eat_soup| 1361|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.select(\"class\").groupby(\"class\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Random Forest model with different hyperparameters listed below and report the best performing hyperparameter combinations.\n",
    "### Use the accuracy metric when evaluating the model with different hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters values\n",
    "numTrees = [10, 20]\n",
    "maxDepth = [5, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty dictionary where hyperparameter and sccuracy combinations (27 in total) will be stored on every iteration\n",
    "dict_hyper = {\"numTrees\" : [], \"maxDepth\" : [], \"accuracy\" : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination 1\n",
      "numTrees: 10\n",
      "maxDepth: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.43895327688579727\n",
      "\n",
      "\n",
      "Combination 2\n",
      "numTrees: 10\n",
      "maxDepth: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.4615341567286545\n",
      "\n",
      "\n",
      "Combination 3\n",
      "numTrees: 20\n",
      "maxDepth: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.44216552596381464\n",
      "\n",
      "\n",
      "Combination 4\n",
      "numTrees: 20\n",
      "maxDepth: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 248:============================>                            (4 + 4) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.46758650407687014\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "iteration = 1\n",
    "\n",
    "for i in numTrees:\n",
    "    for j in maxDepth:\n",
    "        \n",
    "        # Printing current hyperparameters values\n",
    "        print(\"Combination \" + str(iteration))\n",
    "        print(\"numTrees: \" + str(i))\n",
    "        print(\"maxDepth: \" + str(j))\n",
    "        \n",
    "        # Storing current hyperparameters values\n",
    "        dict_hyper[\"numTrees\"].append(i)\n",
    "        dict_hyper[\"maxDepth\"].append(j)\n",
    "        \n",
    "        # Defining model, pipeline and getting accuracy\n",
    "        rf = RandomForestClassifier(numTrees=i, maxDepth=j, labelCol=\"label\", seed=1)\n",
    "        pipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer, rf])\n",
    "        model = pipeline.fit(df_train)\n",
    "        prediction = model.transform(df_train)\n",
    "        binEval = MulticlassClassificationEvaluator(). \\\n",
    "            setMetricName(\"accuracy\"). \\\n",
    "            setPredictionCol(\"prediction\"). \\\n",
    "            setLabelCol(\"label\")\n",
    "        acc_temp = binEval.evaluate(prediction)\n",
    "        \n",
    "        # Appending accuracy result\n",
    "        dict_hyper[\"accuracy\"].append(acc_temp)\n",
    "        print(\"accuracy: \" + str(acc_temp))\n",
    "        iteration += 1\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame using pandas\n",
    "df2 = pd.DataFrame.from_dict(dict_hyper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting pandas DataFrame to spark DataFrame\n",
    "df2 = spark.createDataFrame(df2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 252:=====>                                                 (1 + 10) / 11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------------------+\n",
      "|numTrees|maxDepth|           accuracy|\n",
      "+--------+--------+-------------------+\n",
      "|      10|       5|0.43895327688579727|\n",
      "|      10|       7| 0.4615341567286545|\n",
      "|      20|       5|0.44216552596381464|\n",
      "|      20|       7|0.46758650407687014|\n",
      "+--------+--------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 253:>                                                      (0 + 16) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------------------+\n",
      "|numTrees|maxDepth|           accuracy|\n",
      "+--------+--------+-------------------+\n",
      "|      20|       7|0.46758650407687014|\n",
      "+--------+--------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.sort(desc('accuracy')).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 55.042719,
   "end_time": "2021-01-28T16:00:26.871724",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/jovyan/work/elyra-classification/train-trusted-ai.ipynb",
   "output_path": "/home/jovyan/work/elyra-classification/train-trusted-ai.ipynb",
   "parameters": {},
   "start_time": "2021-01-28T15:59:31.829005",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
