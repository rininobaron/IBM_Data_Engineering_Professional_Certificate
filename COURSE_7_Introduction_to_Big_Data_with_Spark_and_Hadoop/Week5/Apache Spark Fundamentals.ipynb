{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://gitlab.com/ibm/skills-network/courses/placeholder101/-/raw/master/labs/module%201/images/IDSNlogo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"  />\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Getting Started with Apache Spark on IBM Cloud**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **15** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://spark.apache.org/images/spark-logo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Notebook deployed on IBM Watson Studio \n",
    "[Notebook link](https://dataplatform.cloud.ibm.com/analytics/notebooks/v2/43ae90ff-9a69-47b9-a9f0-cde108fef7dc/view?access_token=780ea6ddcecd9f1fc2227fefda074c19dd78011ca820dcb7b7fc42f037a36dc8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will cover some data partioning strategies and methods in Apache Spark and PySpark. We will start with creating the SparkContext and SparkSession. We then create an RDD and apply data partioning. Finally we demonstrate data partioning with dataframes and SparkSQL.\n",
    "\n",
    "After this lab you will be able to:\n",
    "\n",
    "*   Create the SparkContext and SparkSession\n",
    "*   Create an RDD and apply data partioning to it\n",
    "*   Create a dataframe and apply data partioning to it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we are going to be using Python and Spark (PySpark). These libraries should be installed in your lab environment or in SN Labs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20211212081959-0000\n",
      "KERNEL_ID = 3be00dbf-c06f-4ee7-929b-882c76b783dc\n"
     ]
    }
   ],
   "source": [
    "# PySpark is the Spark API for Python. In this lab, we use PySpark to initialize the spark context. \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 -  Spark Context and Spark Session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will create the Spark Context and initialize the Spark session needed for SparkSQL and DataFrames.\n",
    "SparkContext is the entry point for Spark applications and contains functions to create RDDs such as `parallelize()`. SparkSession is needed for SparkSQL and DataFrame operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Creating the spark session and context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a spark context class\n",
    "#sc = SparkContext()\n",
    "sc = SparkContext.getOrCreate();\n",
    "\n",
    "# Creating a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark IBM Cloud Example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Initialize Spark session\n",
    "\n",
    "To work with dataframes we just need to verify that the spark session instance has been created.\n",
    "Feel free to click on the \"Spark UI\" button to explore the Spark UI elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.30.208.180:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://jkg-deployment-3be00dbf-c06f-4ee7-929b-882c76b783dc-684bf5qtt5k:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5101ecccd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: RDDs\n",
    "\n",
    "In this exercise we work with Resilient Distributed Datasets (RDDs). RDDs are Spark's primitive data abstraction and we use concepts from functional programming to create and manipulate RDDs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Create an RDD.\n",
    "\n",
    "For demonstration purposes, we create an RDD here by calling `sc.parallelize()`\\\n",
    "We create an RDD which has integers from 1 to 10.\n",
    "\n",
    "We then get the number of partitions using the `getNumPartitions()` function and the partitions using the `glom()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Partitioner: None\n",
      "Partitions structure: [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "nums = [i for i in range(10)]\n",
    "\n",
    "rdd = sc.parallelize(nums,2)\n",
    "    \n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell we can see the default partitions done for the RDD. However, we can change that by passing in an optional second argument to the `parallelize` function.\n",
    "Let us now try with 2 and 15 partitions and see how they look like in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default parallelism: 2\n",
      "Number of partitions: 2\n",
      "Partitioner: None\n",
      "Partitions structure: [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(nums, 2)\n",
    "    \n",
    "print(\"Default parallelism: {}\".format(sc.defaultParallelism))\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 15\n",
      "Partitioner: None\n",
      "Partitions structure: [[], [0], [1], [], [2], [3], [], [4], [5], [], [6], [7], [], [8], [9]]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(nums, 15)\n",
    "\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anoter way to partition data is by using the `partitionBy()` function. In this case, the dataset needs to be a tuple with a key/value pair as the default partioner uses a hash for the key to assign elements to a parition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Partitioner: <pyspark.rdd.Partitioner object at 0x7f5101e21150>\n",
      "Partitions structure: [[(6, 6), (8, 8), (0, 0), (2, 2), (4, 4)], [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)]]\n",
      "partition: 1 [(6, 6), (8, 8), (0, 0), (2, 2), (4, 4)]\n",
      "partition: 2 [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(nums) \\\n",
    "        .map(lambda el: (el, el)) \\\n",
    "        .partitionBy(2) \\\n",
    "        .persist()\n",
    "    \n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n",
    "\n",
    "j=0\n",
    "for i in rdd.glom().collect():\n",
    "    j+=1\n",
    "    print(\"partition: \" + str(j) + \" \"+ str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that now the elements are distributed differently. A few interesting things happened:\n",
    "\n",
    "```\n",
    "parallelize(nums) - we are transforming Python array into RDD with no partitioning scheme,\n",
    "map(lambda el: (el, el)) - transforming data into the form of a tuple,\n",
    "partitionBy(2) - splitting data into 2 chunks using default hash partitioner\n",
    "```\n",
    "\n",
    "Explicit assignment of partition locations makes the hashing strategy more apparent. The use of the % function assigns it to the correct partition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create a more practical dataset of transactions. We have 8 transactions from 4 different geographies as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = [\n",
    "    {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'},\n",
    "    {'name': 'James', 'amount': 15, 'country': 'United Kingdom'},\n",
    "    {'name': 'Marek', 'amount': 51, 'country': 'Poland'},\n",
    "    {'name': 'Johannes', 'amount': 200, 'country': 'Germany'},\n",
    "    {'name': 'Thomas', 'amount': 30, 'country': 'Germany'},\n",
    "    {'name': 'Paul', 'amount': 75, 'country': 'Poland'},\n",
    "    {'name': 'Pierre', 'amount': 120, 'country': 'France'},\n",
    "    {'name': 'Frank', 'amount': 180, 'country': 'France'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that further analysis will be performed analyzing many similar records within the same country. To optimize network traffic it seems to be a good idea to put records from one country in one node. To meet this requirement, we will need a custom partitioner: Custom partitioner — function returning an integer for given object (tuple key).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589857\n",
      "4384563\n",
      "5395502\n",
      "5326111\n"
     ]
    }
   ],
   "source": [
    "# Dummy implementation assuring that data for each country is in one partition\n",
    "def country_partitioner(country):\n",
    "    return hash(country)% (10**7+1)\n",
    "    #return portable_hash(country)\n",
    "    \n",
    "\n",
    "# Validate results\n",
    "print(country_partitioner(\"Poland\"))\n",
    "print(country_partitioner(\"Germany\"))\n",
    "print(country_partitioner(\"United Kingdom\"))\n",
    "print(country_partitioner(\"France\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our custom partitioner creates a unique hash for each country name so it can be used to `partitionBy` our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 5\n",
      "Partitioner: <pyspark.rdd.Partitioner object at 0x7f50ecfc8150>\n",
      "Partitions structure: [[('United Kingdom', {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'}), ('United Kingdom', {'name': 'James', 'amount': 15, 'country': 'United Kingdom'})], [('France', {'name': 'Pierre', 'amount': 120, 'country': 'France'}), ('France', {'name': 'Frank', 'amount': 180, 'country': 'France'})], [('Germany', {'name': 'Thomas', 'amount': 30, 'country': 'Germany'}), ('Germany', {'name': 'Johannes', 'amount': 200, 'country': 'Germany'})], [('Poland', {'name': 'Paul', 'amount': 75, 'country': 'Poland'}), ('Poland', {'name': 'Marek', 'amount': 51, 'country': 'Poland'})], []]\n",
      "\n",
      "--\n",
      "\n",
      "\n",
      "partition: 1\n",
      "[('United Kingdom', {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'}), ('United Kingdom', {'name': 'James', 'amount': 15, 'country': 'United Kingdom'})]\n",
      "\n",
      "partition: 2\n",
      "[('France', {'name': 'Pierre', 'amount': 120, 'country': 'France'}), ('France', {'name': 'Frank', 'amount': 180, 'country': 'France'})]\n",
      "\n",
      "partition: 3\n",
      "[('Germany', {'name': 'Thomas', 'amount': 30, 'country': 'Germany'}), ('Germany', {'name': 'Johannes', 'amount': 200, 'country': 'Germany'})]\n",
      "\n",
      "partition: 4\n",
      "[('Poland', {'name': 'Marek', 'amount': 51, 'country': 'Poland'}), ('Poland', {'name': 'Paul', 'amount': 75, 'country': 'Poland'})]\n",
      "\n",
      "partition: 5\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(transactions) \\\n",
    "        .map(lambda el: (el['country'], el)) \\\n",
    "        .partitionBy(5, country_partitioner)\n",
    "    \n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n",
    "\n",
    "print(\"\\n--\\n\")\n",
    "for i, j in enumerate(rdd.glom().collect()):\n",
    "    print(\"\\npartition: \" + str(i+1) + \"\\n\"+ str(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the partitioning scheme, we can now carry out calculations such as total revenue/sales as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_sales(iterator):\n",
    "    yield sum(transaction[1]['amount'] for transaction in iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions structure: [[('United Kingdom', {'name': 'Bob', 'amount': 100, 'country': 'United Kingdom'}), ('United Kingdom', {'name': 'James', 'amount': 15, 'country': 'United Kingdom'})], [('France', {'name': 'Pierre', 'amount': 120, 'country': 'France'}), ('France', {'name': 'Frank', 'amount': 180, 'country': 'France'})], [('Germany', {'name': 'Thomas', 'amount': 30, 'country': 'Germany'}), ('Germany', {'name': 'Johannes', 'amount': 200, 'country': 'Germany'})], [('Poland', {'name': 'Paul', 'amount': 75, 'country': 'Poland'}), ('Poland', {'name': 'Marek', 'amount': 51, 'country': 'Poland'})], []]\n",
      "Total sales for each partition: [115, 300, 230, 126, 0]\n"
     ]
    }
   ],
   "source": [
    "by_country = sc.parallelize(transactions) \\\n",
    "        .map(lambda el: (el['country'], el)) \\\n",
    "        .partitionBy(5, country_partitioner)\n",
    "    \n",
    "print(\"Partitions structure: {}\".format(by_country.glom().collect()))\n",
    "\n",
    "# Sum sales in each partition\n",
    "sum_amounts = by_country \\\n",
    "    .mapPartitions(sum_sales) \\\n",
    "    .collect()\n",
    "\n",
    "print(\"Total sales for each partition: {}\".format(sum_amounts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: DataFrames\n",
    "\n",
    "In this exercise we work with DataFrames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Create the DataFrame\n",
    "\n",
    "We will now create a DataFrame from the previous \"transactions\" list we created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ibm/conda/miniconda/lib/python/site-packages/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------+\n",
      "|amount|       country|    name|\n",
      "+------+--------------+--------+\n",
      "|   100|United Kingdom|     Bob|\n",
      "|    15|United Kingdom|   James|\n",
      "|    51|        Poland|   Marek|\n",
      "|   200|       Germany|Johannes|\n",
      "|    30|       Germany|  Thomas|\n",
      "|    75|        Poland|    Paul|\n",
      "|   120|        France|  Pierre|\n",
      "|   180|        France|   Frank|\n",
      "+------+--------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Partitioner: <pyspark.rdd.Partitioner object at 0x7f50ecfc8150>\n",
      "Partitions structure: [[Row(amount=100, country='United Kingdom', name='Bob'), Row(amount=15, country='United Kingdom', name='James'), Row(amount=51, country='Poland', name='Marek'), Row(amount=200, country='Germany', name='Johannes')], [Row(amount=30, country='Germany', name='Thomas'), Row(amount=75, country='Poland', name='Paul'), Row(amount=120, country='France', name='Pierre'), Row(amount=180, country='France', name='Frank')]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions: {}\".format(df.rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(df.rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition: 1\n",
      "[Row(amount=100, country='United Kingdom', name='Bob'), Row(amount=15, country='United Kingdom', name='James'), Row(amount=51, country='Poland', name='Marek'), Row(amount=200, country='Germany', name='Johannes')]\n",
      "partition: 2\n",
      "[Row(amount=30, country='Germany', name='Thomas'), Row(amount=75, country='Poland', name='Paul'), Row(amount=120, country='France', name='Pierre'), Row(amount=180, country='France', name='Frank')]\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(df.rdd.glom().collect()):\n",
    "    print(\"partition: \" + str(i+1) + \"\\n\"+ str(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dataframes we can repartition the dataset by column using the `repartition()` function. The cell below shows how we can partition the dataset by country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After 'repartition()'\n",
      "Number of partitions: 10\n",
      "Partitioner: None\n",
      "Partitions structure: [[Row(amount=120, country='France', name='Pierre'), Row(amount=180, country='France', name='Frank')], [], [Row(amount=200, country='Germany', name='Johannes'), Row(amount=30, country='Germany', name='Thomas')], [], [Row(amount=75, country='Poland', name='Paul'), Row(amount=51, country='Poland', name='Marek')], [Row(amount=100, country='United Kingdom', name='Bob'), Row(amount=15, country='United Kingdom', name='James')], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# Repartition by column\n",
    "df2 = df.repartition(10,\"country\")\n",
    "\n",
    "print(\"\\nAfter 'repartition()'\")\n",
    "print(\"Number of partitions: {}\".format(df2.rdd.getNumPartitions()))\n",
    "print(\"Partitioner: {}\".format(df2.rdd.partitioner))\n",
    "print(\"Partitions structure: {}\".format(df2.rdd.glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition: 1\n",
      "[Row(amount=120, country='France', name='Pierre'), Row(amount=180, country='France', name='Frank')]\n",
      "partition: 2\n",
      "[]\n",
      "partition: 3\n",
      "[Row(amount=200, country='Germany', name='Johannes'), Row(amount=30, country='Germany', name='Thomas')]\n",
      "partition: 4\n",
      "[]\n",
      "partition: 5\n",
      "[Row(amount=75, country='Poland', name='Paul'), Row(amount=51, country='Poland', name='Marek')]\n",
      "partition: 6\n",
      "[Row(amount=100, country='United Kingdom', name='Bob'), Row(amount=15, country='United Kingdom', name='James')]\n",
      "partition: 7\n",
      "[]\n",
      "partition: 8\n",
      "[]\n",
      "partition: 9\n",
      "[]\n",
      "partition: 10\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(df2.rdd.glom().collect()):\n",
    "    print(\"partition: \" + str(i+1) + \"\\n\"+ str(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some of the partitioning strategies for dataframes and RDDs. As the data size increases, the paritioning strategies become more important and can yield significant performance benefits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Karthik Muthuraman](https://www.linkedin.com/in/karthik-muthuraman/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMBD0225ENSkillsNetwork25716109-2021-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jerome Nilmeier](https://github.com/nilmeier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description |\n",
    "| ----------------- | ------- | ---------- | ------------------ |\n",
    "| 2021-07-02        | 0.2     | Karthik    | Beta launch        |\n",
    "| 2021-06-30        | 0.1     | Karthik    | First Draft        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2021 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
